{
  "issues": [
    {
      "title": "[Tracing] Add Workflow Node Observability - Phase/Step Visibility",
      "priority": "urgent",
      "estimate": 4,
      "labels": ["tracing", "langfuse", "observability", "critical"],
      "description": "## Goal\nMake the 4 main workflow phases (scope, fill, research, finalize) visible in Langfuse traces by adding @observe decorators.\n\n## Problem\nCurrently, Langfuse traces only show the top-level wrapper and individual tool calls. The 4 workflow phases are completely invisible, making it impossible to:\n- See which phase is executing\n- Measure time per phase\n- Identify phase-specific bottlenecks\n- Debug phase failures\n\n## Solution\nAdd `@observe(as_type=\"span\")` decorators to all 4 workflow node functions in `core/graph.py`:\n1. `scope()` - Strategy selection & categorization\n2. `fill()` - Variable resolution\n3. `research()` - Main research orchestration\n4. `finalize()` - Report synthesis\n\n## Implementation (Validated with Langfuse v3 Docs)\n\nPer official docs: Use `@observe(as_type=\"span\")` for workflow operations and `update_current_span()` for metadata.\n\n### Example for scope() function:\n\n```python\nfrom core.langfuse_tracing import observe, get_langfuse_client\n\n@observe(as_type=\"span\", name=\"scope-phase\")\ndef scope(state: State) -> State:\n    lf_client = get_langfuse_client()\n    \n    # Capture input at start\n    if lf_client:\n        lf_client.update_current_span(\n            input={\n                \"user_request\": state.user_request[:200],\n                \"existing_strategy\": state.strategy_slug\n            },\n            metadata={\n                \"phase\": \"scope\",\n                \"has_category\": bool(state.category)\n            }\n        )\n    \n    # ... existing logic ...\n    \n    # Capture output at end\n    if lf_client:\n        lf_client.update_current_span(\n            output={\n                \"strategy_slug\": state.strategy_slug,\n                \"category\": state.category,\n                \"tasks_count\": len(state.tasks)\n            },\n            metadata={\n                \"strategy_selected\": state.strategy_slug\n            }\n        )\n    \n    return state\n```\n\nRepeat pattern for fill(), research(), finalize().\n\n## Files to Modify\n- `core/graph.py` - Add @observe + update calls to 4 functions\n\n## Acceptance Criteria\n- [ ] All 4 nodes have @observe(as_type=\"span\") decorators\n- [ ] Input captured at start of each phase\n- [ ] Output captured at end of each phase\n- [ ] Metadata includes phase name and strategy\n- [ ] Langfuse shows 4 distinct phase spans with timing\n- [ ] No performance degradation (< 50ms overhead/phase)\n- [ ] Nesting works: phases appear under parent trace\n\n## Testing\n```bash\npython3 run_daily_briefing.py --topic \"AI news\" --debug\n# Check Langfuse dashboard for 4 visible phase spans\n```\n\n## Expected Impact\n**Before**: 5% visibility (only wrapper + tool calls)\n**After**: 60% visibility (phases + tool calls)\n\n## Technical Reference\n- Langfuse Docs: https://langfuse.com/docs/sdk/python/decorators\n- Nesting: Automatic via OpenTelemetry context propagation"
    },
    {
      "title": "[Tracing] Add Research Step-Level Tracing - Individual Step Visibility",
      "priority": "urgent",
      "estimate": 6,
      "labels": ["tracing", "langfuse", "observability", "critical"],
      "description": "## Goal\nMake individual research steps visible as nested spans within the research phase.\n\n## Problem\nThe research() phase executes 5-15 steps per strategy, but none are visible in traces. Cannot:\n- See which step is executing\n- Identify slow steps\n- Debug skipped steps (when conditions)\n- Track tool usage per step\n\n## Solution\nAdd nested span creation for each step in the research loop using `langfuse.start_as_current_span()` context manager.\n\n## Implementation (Validated with Langfuse v3 Docs)\n\nPer official docs: \"Nesting is handled automatically by OpenTelemetry's context propagation.\"\n\n### Pattern:\n\n```python\n# In research() function, around line 486\n\nfor idx, step in enumerate(research_steps):\n    step_label = step.get(\"use\") or step.get(\"name\") or f\"step-{idx}\"\n    \n    lf_client = get_langfuse_client()\n    if lf_client:\n        with lf_client.start_as_current_span(\n            name=f\"research-step-{idx+1}:{step_label}\"\n        ) as step_span:\n            # Capture input\n            step_span.update(\n                input={\n                    \"step_index\": idx + 1,\n                    \"step_name\": step_label,\n                    \"tool\": step.get(\"use\"),\n                    \"params\": {k: str(v)[:100] for k, v in step.get(\"params\", {}).items()}\n                },\n                metadata={\n                    \"strategy\": state.strategy_slug,\n                    \"step_index\": idx + 1\n                }\n            )\n            \n            # Handle when condition\n            if step.get(\"when\") and not _eval_when(step[\"when\"], state):\n                step_span.update(\n                    output={\"skipped\": True, \"reason\": \"when_condition_false\"},\n                    metadata={\"skipped\": True}\n                )\n                continue\n            \n            # Execute step (existing code)\n            try:\n                # ... existing step execution ...\n                \n                step_span.update(\n                    output={\n                        \"results_count\": len(results),\n                        \"skipped\": False\n                    },\n                    metadata={\"execution_success\": True}\n                )\n            except Exception as e:\n                step_span.update(\n                    output={\"error\": str(e)},\n                    level=\"ERROR\"\n                )\n                raise\n    else:\n        # Fallback: execute without tracing\n```\n\n## Files to Modify\n- `core/graph.py:486` - research() function step loop\n\n## Acceptance Criteria\n- [ ] Each step creates nested span under research-phase\n- [ ] Step index and name visible\n- [ ] Skipped steps marked with reason\n- [ ] Tool parameters captured (truncated to 100 chars)\n- [ ] Results count captured\n- [ ] Errors captured with ERROR level\n- [ ] Step timing visible in Langfuse\n- [ ] Proper nesting hierarchy verified\n\n## Testing\n```bash\npython3 run_daily_briefing.py --topic \"Financial news\" --debug\n# Verify steps appear as children of research-phase span\n# Check skipped steps show skip reason\n```\n\n## Expected Impact\n**Before**: Cannot see step execution\n**After**: 80% visibility - see every step, timing, results\n\n## Dependencies\n- Requires Issue 1 (Workflow Node Tracing) completed first\n\n## Technical Reference\n- Context Managers: https://langfuse.com/docs/sdk/python/decorators#nesting"
    },
    {
      "title": "[Tracing] Add Comprehensive LLM Call Tracing - Token & Cost Visibility",
      "priority": "high",
      "estimate": 8,
      "labels": ["tracing", "langfuse", "llm", "cost-optimization"],
      "description": "## Goal\nMake ALL LLM calls visible with prompts, responses, and token usage for cost optimization.\n\n## Problem\nCurrently only 2 of ~10 LLM calls are traced:\n- ✅ Evidence clustering (graph.py:149)\n- ✅ Query refinement (graph.py:237)\n- ❌ Scope analysis LLM\n- ❌ Finalize analysis LLM\n- ❌ Finalize writer LLM\n- ❌ Other LLM calls in llm_analyzer.py\n\nCannot optimize prompts or track costs.\n\n## Solution\nAdd `@observe(as_type=\"generation\")` to all LLM call functions with token tracking.\n\n## Implementation (Validated with Langfuse v3 Docs)\n\nPer official docs: Use `as_type=\"generation\"` for LLM calls and include `usage_details`.\n\n### Pattern:\n\n```python\n@observe(as_type=\"generation\", name=\"scope-analysis-llm\")\ndef scope_request(user_request: str) -> Dict[str, Any]:\n    lf_client = get_langfuse_client()\n    \n    prompt = f\"Analyze: {user_request}\"\n    \n    # Capture input\n    if lf_client:\n        lf_client.update_current_generation(\n            model=\"gpt-4\",\n            input={\"prompt\": prompt, \"user_request\": user_request[:500]},\n            metadata={\"purpose\": \"scope_analysis\"}\n        )\n    \n    # Make LLM call\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    result = response.choices[0].message.content\n    \n    # Capture output + tokens\n    if lf_client:\n        lf_client.update_current_generation(\n            output=result,\n            usage_details={\n                \"input_tokens\": response.usage.prompt_tokens,\n                \"output_tokens\": response.usage.completion_tokens,\n                \"total_tokens\": response.usage.total_tokens\n            }\n        )\n    \n    return parse_result(result)\n```\n\n## Files to Modify\n1. `core/scope.py` - scope_request() function\n2. `core/graph.py` - finalize() analysis & writer LLM calls\n3. `core/llm_analyzer.py` - All analysis functions\n\n## Locations to Add @observe\n1. `core/scope.py:scope_request()` - Strategy selection\n2. `core/graph.py:finalize()` - Analysis LLM\n3. `core/graph.py:finalize()` - Writer LLM\n4. `core/llm_analyzer.py` - Any LLM functions\n5. Already done: `_cluster_llm()`, `_refine_queries_with_llm()`\n\n## Acceptance Criteria\n- [ ] All LLM calls have @observe(as_type=\"generation\")\n- [ ] Prompts captured in input\n- [ ] Responses captured in output\n- [ ] Token usage in usage_details {input_tokens, output_tokens, total_tokens}\n- [ ] Model name specified\n- [ ] Purpose/context in metadata\n- [ ] No duplicate tracing (avoid double-wrapping)\n- [ ] Generations nest under parent spans\n\n## Testing\n```bash\npython3 run_daily_briefing.py --topic \"Tech news\" --debug\n# Verify all LLM calls appear as generations in Langfuse\n# Check token counts match actual usage\n# Calculate total cost from token usage\n```\n\n## Expected Impact\n**Before**: 2/10 LLM calls visible, no token data\n**After**: 10/10 LLM calls visible, full cost tracking\n\n## Cost Optimization Value\n- Identify most expensive prompts\n- A/B test prompt variations\n- Track cost per research task\n- Optimize token usage\n\n## Technical Reference\n- Generation Type: https://langfuse.com/docs/sdk/python/decorators#generation-type\n- Usage Details: Token tracking for cost calculation"
    },
    {
      "title": "[Tracing] Add Strategy Selection Tracing - Debug Strategy Matching",
      "priority": "high",
      "estimate": 4,
      "labels": ["tracing", "langfuse", "strategy", "debugging"],
      "description": "## Goal\nMake strategy selection process visible with all candidates and scoring logic.\n\n## Problem\nWhen wrong strategy is selected, cannot debug:\n- What strategies were considered?\n- What scores did they get?\n- Why was one chosen over another?\n- What variables were extracted?\n\nStrategy selection is completely opaque.\n\n## Solution\nAdd `@observe(as_type=\"span\")` to select_strategy() function with candidate tracking.\n\n## Implementation (Validated with Langfuse v3 Docs)\n\n```python\n# In strategies/__init__.py\n\nfrom core.langfuse_tracing import observe, get_langfuse_client\n\n@observe(as_type=\"span\", name=\"strategy-selection\")\ndef select_strategy(\n    category: str,\n    time_window: str,\n    depth: str,\n    user_request: str = \"\"\n) -> str:\n    lf_client = get_langfuse_client()\n    \n    # Capture input criteria\n    if lf_client:\n        lf_client.update_current_span(\n            input={\n                \"category\": category,\n                \"time_window\": time_window,\n                \"depth\": depth,\n                \"user_request\": user_request[:200]\n            },\n            metadata={\n                \"category\": category,\n                \"time_window\": time_window,\n                \"depth\": depth\n            }\n        )\n    \n    # Get all strategies and score them\n    all_strategies = get_all_strategies()\n    \n    candidates = []\n    for slug, strategy in all_strategies.items():\n        score = _calculate_match_score(strategy, category, time_window, depth)\n        candidates.append({\n            \"slug\": slug,\n            \"score\": score,\n            \"category\": strategy.category\n        })\n    \n    candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n    selected = candidates[0][\"slug\"] if candidates else \"default\"\n    \n    # Capture output with top candidates\n    if lf_client:\n        lf_client.update_current_span(\n            output={\n                \"selected_strategy\": selected,\n                \"top_candidates\": candidates[:5],  # Top 5\n                \"total_candidates\": len(candidates)\n            },\n            metadata={\n                \"strategy_selected\": selected,\n                \"match_score\": candidates[0][\"score\"] if candidates else 0\n            }\n        )\n    \n    return selected\n```\n\n## Files to Modify\n- `strategies/__init__.py` - select_strategy() and related functions\n\n## Acceptance Criteria\n- [ ] Strategy selection creates span under scope-phase\n- [ ] Selection criteria captured (category, time_window, depth)\n- [ ] Top 5 candidate strategies with scores captured\n- [ ] Selected strategy in output\n- [ ] Match score captured in metadata\n- [ ] Proper nesting under scope-phase span\n\n## Testing\n```bash\npython3 run_daily_briefing.py --topic \"News from yesterday\"\n# Verify strategy-selection span appears\n# Check candidates list shows alternatives\n# Confirm selected strategy matches expected\n```\n\n## Expected Impact\n**Before**: Strategy selection is a black box\n**After**: Full visibility into matching logic and alternatives\n\n## Debugging Value\n- Understand why strategies are chosen\n- Identify misclassifications\n- Optimize scoring algorithm\n- Validate strategy coverage\n\n## Dependencies\n- Requires Issue 1 (Workflow Node Tracing) for proper nesting\n\n## Technical Reference\n- Span Type: https://langfuse.com/docs/sdk/python/decorators#span-type"
    },
    {
      "title": "[Tracing] Add Error Path Tracing & Progress Tracking Metadata",
      "priority": "medium",
      "estimate": 6,
      "labels": ["tracing", "langfuse", "error-handling", "ux"],
      "description": "## Goal\nCapture errors with full context and add progress tracking for long-running tasks.\n\n## Problem\n**Error Handling**: When failures occur, cannot see:\n- Which phase/step failed?\n- What was the error?\n- What was the state at failure?\n- Was there partial progress?\n\n**Progress Tracking**: Users waiting for research have no visibility into progress.\n\n## Solution\n1. Add error tracing to all operations with ERROR level\n2. Add progress metadata updates during execution\n\n## Implementation (Validated with Langfuse v3 Docs)\n\n### Error Tracing Pattern:\n\n```python\n# In all workflow nodes and steps\n\ntry:\n    # ... operation ...\n    \nexcept Exception as e:\n    lf_client = get_langfuse_client()\n    if lf_client:\n        lf_client.update_current_span(\n            output={\n                \"error\": str(e),\n                \"error_type\": type(e).__name__\n            },\n            metadata={\n                \"error_occurred\": True,\n                \"error_type\": type(e).__name__,\n                \"error_message\": str(e)[:500]\n            },\n            level=\"ERROR\"\n        )\n    raise\n```\n\n### Progress Tracking (in api/main.py):\n\n```python\n# During batch execution\n\nfor idx, task in enumerate(tasks, 1):\n    with workflow_span(...) as trace_ctx:\n        # Update progress\n        trace_ctx.update_trace(\n            metadata={\n                \"progress\": {\n                    \"current_task\": idx,\n                    \"total_tasks\": len(tasks),\n                    \"percent_complete\": int((idx / len(tasks)) * 100)\n                }\n            }\n        )\n        \n        # ... execute research ...\n```\n\n## Files to Modify\n1. `core/graph.py` - Add try/except to all 4 nodes + step loop\n2. `api/main.py` - Add progress metadata updates\n3. All spans/generations from Issues 1-4\n\n## Acceptance Criteria\n- [ ] All exceptions caught and traced with ERROR level\n- [ ] Error type and message captured\n- [ ] No silent failures (all errors visible)\n- [ ] Progress metadata shows current_task/total_tasks\n- [ ] Progress percentage calculated correctly\n- [ ] Langfuse shows ERROR level observations\n- [ ] Stack traces available for debugging\n\n## Testing\n\n### Error Scenarios:\n```bash\n# Test invalid topic\npython3 run_daily_briefing.py --topic \"!!!INVALID!!!\" --debug\n# Verify error captured with details\n\n# Test network failure (disconnect)\n# Verify error trace shows which tool failed\n```\n\n### Progress Tracking:\n```bash\n# Trigger batch with multiple tasks via API\n# Monitor Langfuse for progress metadata updates\n# Verify percent_complete increments correctly\n```\n\n## Expected Impact\n**Error Debugging**:\n- Before: \"It failed\" (no context)\n- After: See exact failure point, error type, state\n\n**Progress UX**:\n- Before: Silent execution\n- After: Real-time progress visibility\n\n## Dependencies\n- Requires Issues 1-4 (all tracing infrastructure in place)\n\n## Technical Reference\n- Error Level: https://langfuse.com/docs/sdk/python/decorators#error-handling\n- Metadata: Alphanumeric keys, ≤200 char values"
    }
  ]
}
