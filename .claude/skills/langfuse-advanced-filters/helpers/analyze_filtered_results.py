#!/usr/bin/env python3
"""
Analyze Filtered Langfuse Results

Extracts insights from query results generated by query_with_filters.py

Usage:
    # Latency breakdown analysis
    python3 analyze_filtered_results.py \
      --input /tmp/langfuse_queries/slow_traces.json \
      --analysis-type latency-breakdown

    # Compare two result sets
    python3 analyze_filtered_results.py \
      --input /tmp/langfuse_queries/before.json \
      --compare /tmp/langfuse_queries/after.json \
      --analysis-type comparison
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, Any, List
from collections import Counter
from datetime import datetime


def load_results(file_path: str) -> Dict[str, Any]:
    """Load query results from JSON file."""
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading file {file_path}: {e}", file=sys.stderr)
        sys.exit(1)


def analyze_latency_breakdown(results: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze latency distribution and patterns."""
    data = results.get('results', [])

    if not data:
        return {'error': 'No results to analyze'}

    latencies = []
    for item in data:
        # Try to extract latency from various fields
        latency = None
        if 'latency' in item:
            latency = item['latency']
        elif 'end_time' in item and 'start_time' in item:
            # Calculate latency
            try:
                start = datetime.fromisoformat(item['start_time'].replace('Z', '+00:00'))
                end = datetime.fromisoformat(item['end_time'].replace('Z', '+00:00'))
                latency = (end - start).total_seconds() * 1000  # ms
            except:
                pass

        if latency is not None:
            latencies.append(latency)

    if not latencies:
        return {'error': 'No latency data found'}

    # Calculate statistics
    latencies.sort()
    n = len(latencies)

    analysis = {
        'count': n,
        'min': latencies[0],
        'max': latencies[-1],
        'mean': sum(latencies) / n,
        'median': latencies[n // 2],
        'p95': latencies[int(n * 0.95)] if n > 0 else 0,
        'p99': latencies[int(n * 0.99)] if n > 0 else 0,
        'distribution': {
            '<1s': sum(1 for l in latencies if l < 1000),
            '1-3s': sum(1 for l in latencies if 1000 <= l < 3000),
            '3-5s': sum(1 for l in latencies if 3000 <= l < 5000),
            '5-10s': sum(1 for l in latencies if 5000 <= l < 10000),
            '>10s': sum(1 for l in latencies if l >= 10000)
        }
    }

    return analysis


def analyze_metadata_patterns(results: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze metadata patterns across results."""
    data = results.get('results', [])

    if not data:
        return {'error': 'No results to analyze'}

    # Collect all metadata keys and values
    metadata_stats = {}

    for item in data:
        metadata = item.get('metadata', {})
        for key, value in metadata.items():
            if key not in metadata_stats:
                metadata_stats[key] = Counter()
            metadata_stats[key][str(value)] += 1

    # Convert to analyzable format
    analysis = {}
    for key, counter in metadata_stats.items():
        analysis[key] = {
            'unique_values': len(counter),
            'most_common': counter.most_common(5)
        }

    return analysis


def analyze_error_patterns(results: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze error patterns in results."""
    data = results.get('results', [])

    if not data:
        return {'error': 'No results to analyze'}

    # Collect errors
    error_messages = []
    error_by_name = Counter()

    for item in data:
        # Check for error level
        if item.get('level') == 'ERROR':
            name = item.get('name', 'unknown')
            error_by_name[name] += 1

            # Extract error message
            status_msg = item.get('status_message')
            if status_msg:
                error_messages.append({
                    'trace_id': item.get('trace_id'),
                    'name': name,
                    'message': status_msg
                })

    analysis = {
        'total_errors': sum(error_by_name.values()),
        'errors_by_component': dict(error_by_name.most_common()),
        'sample_messages': error_messages[:10]  # Top 10
    }

    return analysis


def compare_result_sets(before: Dict[str, Any], after: Dict[str, Any]) -> Dict[str, Any]:
    """Compare two result sets (e.g., before/after a change)."""
    before_data = before.get('results', [])
    after_data = after.get('results', [])

    # Compare counts
    comparison = {
        'before_count': len(before_data),
        'after_count': len(after_data),
        'change': len(after_data) - len(before_data),
        'change_pct': ((len(after_data) - len(before_data)) / len(before_data) * 100) if before_data else 0
    }

    # Compare latency if available
    before_latency = analyze_latency_breakdown(before)
    after_latency = analyze_latency_breakdown(after)

    if 'error' not in before_latency and 'error' not in after_latency:
        comparison['latency_comparison'] = {
            'before_mean': before_latency['mean'],
            'after_mean': after_latency['mean'],
            'mean_change': after_latency['mean'] - before_latency['mean'],
            'mean_change_pct': ((after_latency['mean'] - before_latency['mean']) / before_latency['mean'] * 100) if before_latency['mean'] > 0 else 0,
            'before_p95': before_latency['p95'],
            'after_p95': after_latency['p95'],
            'p95_change': after_latency['p95'] - before_latency['p95']
        }

    return comparison


def generate_report(analysis: Dict[str, Any], analysis_type: str) -> str:
    """Generate markdown report from analysis."""
    report = [
        "# Filtered Results Analysis",
        "",
        f"**Analysis Type**: {analysis_type}",
        f"**Generated**: {datetime.now().isoformat()}",
        "",
        "---",
        ""
    ]

    if analysis_type == 'latency-breakdown':
        report.extend([
            "## Latency Analysis",
            "",
            f"**Total Traces**: {analysis.get('count', 0)}",
            "",
            "### Statistics",
            f"- **Mean**: {analysis.get('mean', 0):.1f}ms",
            f"- **Median**: {analysis.get('median', 0):.1f}ms",
            f"- **Min**: {analysis.get('min', 0):.1f}ms",
            f"- **Max**: {analysis.get('max', 0):.1f}ms",
            f"- **P95**: {analysis.get('p95', 0):.1f}ms",
            f"- **P99**: {analysis.get('p99', 0):.1f}ms",
            "",
            "### Distribution",
            ""
        ])

        dist = analysis.get('distribution', {})
        for bucket, count in dist.items():
            pct = (count / analysis['count'] * 100) if analysis.get('count', 0) > 0 else 0
            report.append(f"- **{bucket}**: {count} ({pct:.1f}%)")

    elif analysis_type == 'metadata-patterns':
        report.extend([
            "## Metadata Patterns",
            ""
        ])

        for key, stats in analysis.items():
            report.extend([
                f"### {key}",
                f"**Unique values**: {stats['unique_values']}",
                "",
                "**Most common**:",
                ""
            ])
            for value, count in stats['most_common']:
                report.append(f"- {value}: {count}")
            report.append("")

    elif analysis_type == 'error-patterns':
        report.extend([
            "## Error Analysis",
            "",
            f"**Total Errors**: {analysis.get('total_errors', 0)}",
            "",
            "### Errors by Component",
            ""
        ])

        for component, count in analysis.get('errors_by_component', {}).items():
            report.append(f"- **{component}**: {count}")

        report.extend([
            "",
            "### Sample Error Messages",
            ""
        ])

        for i, error in enumerate(analysis.get('sample_messages', [])[:5], 1):
            report.extend([
                f"**{i}. {error['name']}** (Trace: {error['trace_id'][:8]}...)",
                f"```",
                f"{error['message']}",
                f"```",
                ""
            ])

    elif analysis_type == 'comparison':
        report.extend([
            "## Before/After Comparison",
            "",
            "### Result Counts",
            f"- **Before**: {analysis.get('before_count', 0)}",
            f"- **After**: {analysis.get('after_count', 0)}",
            f"- **Change**: {analysis.get('change', 0):+d} ({analysis.get('change_pct', 0):+.1f}%)",
            ""
        ])

        if 'latency_comparison' in analysis:
            lat = analysis['latency_comparison']
            report.extend([
                "### Latency Comparison",
                "",
                "**Mean Latency**:",
                f"- Before: {lat['before_mean']:.1f}ms",
                f"- After: {lat['after_mean']:.1f}ms",
                f"- Change: {lat['mean_change']:+.1f}ms ({lat['mean_change_pct']:+.1f}%)",
                "",
                "**P95 Latency**:",
                f"- Before: {lat['before_p95']:.1f}ms",
                f"- After: {lat['after_p95']:.1f}ms",
                f"- Change: {lat['p95_change']:+.1f}ms",
                ""
            ])

    return "\n".join(report)


def main():
    parser = argparse.ArgumentParser(
        description='Analyze filtered Langfuse results',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('--input', required=True,
                       help='Input results file (from query_with_filters.py)')
    parser.add_argument('--analysis-type', required=True,
                       choices=['latency-breakdown', 'metadata-patterns', 'error-patterns', 'comparison'],
                       help='Type of analysis to perform')
    parser.add_argument('--compare', type=str,
                       help='Second results file for comparison (required for comparison analysis)')
    parser.add_argument('--output', type=str,
                       help='Output file path (optional, prints to stdout if not specified)')

    args = parser.parse_args()

    # Load results
    results = load_results(args.input)

    # Perform analysis
    if args.analysis_type == 'latency-breakdown':
        analysis = analyze_latency_breakdown(results)
    elif args.analysis_type == 'metadata-patterns':
        analysis = analyze_metadata_patterns(results)
    elif args.analysis_type == 'error-patterns':
        analysis = analyze_error_patterns(results)
    elif args.analysis_type == 'comparison':
        if not args.compare:
            print("Error: --compare required for comparison analysis", file=sys.stderr)
            sys.exit(1)
        before_results = results
        after_results = load_results(args.compare)
        analysis = compare_result_sets(before_results, after_results)

    # Generate report
    report = generate_report(analysis, args.analysis_type)

    # Output
    if args.output:
        output_file = Path(args.output)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w') as f:
            f.write(report)
        print(f"✓ Report saved to: {args.output}")
    else:
        print(report)

    # Also save JSON analysis
    if args.output:
        json_output = Path(args.output).with_suffix('.json')
        with open(json_output, 'w') as f:
            json.dump(analysis, f, indent=2, default=str)
        print(f"✓ JSON analysis saved to: {json_output}")


if __name__ == '__main__':
    main()
